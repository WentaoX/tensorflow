{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 2\n",
    "------------\n",
    "\n",
    "Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 14.609759\n",
      "Training accuracy: 10.6%\n",
      "Validation accuracy: 16.2%\n",
      "Loss at step 100: 2.300517\n",
      "Training accuracy: 72.0%\n",
      "Validation accuracy: 71.1%\n",
      "Loss at step 200: 1.878678\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 73.3%\n",
      "Loss at step 300: 1.644460\n",
      "Training accuracy: 76.1%\n",
      "Validation accuracy: 73.9%\n",
      "Loss at step 400: 1.481197\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 74.4%\n",
      "Loss at step 500: 1.357657\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 74.7%\n",
      "Loss at step 600: 1.259763\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 74.9%\n",
      "Loss at step 700: 1.179735\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 75.1%\n",
      "Loss at step 800: 1.112746\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 75.1%\n",
      "Test accuracy: 82.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x68f-hxRGm3H"
   },
   "source": [
    "Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmVZESmtG4JH"
   },
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 19.027575\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 12.8%\n",
      "Minibatch loss at step 500: 1.727908\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 1000: 1.206405\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 1500: 1.105024\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 2000: 0.766009\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 2500: 0.957891\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 3000: 0.705097\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 78.1%\n",
      "Test accuracy: 84.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Problem\n",
    "-------\n",
    "\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "relu_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, relu_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([relu_nodes]))\n",
    "  \n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([relu_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  # Training computation.\n",
    "  logits1 = tf.matmul(tf_train_dataset, weights1) + biases1\n",
    "  hidden = tf.nn.relu(logits1)\n",
    "  logits2 = tf.matmul(hidden, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits2)\n",
    "    \n",
    "  valid_logits1 = tf.matmul(tf_valid_dataset, weights1) + biases1\n",
    "  valid_hidden = tf.nn.relu(valid_logits1)\n",
    "  valid_logits2 = tf.matmul(valid_hidden, weights2) + biases2\n",
    "  valid_prediction = tf.nn.softmax(valid_logits2)\n",
    "    \n",
    "  test_logits1 = tf.matmul(tf_test_dataset, weights1) + biases1\n",
    "  test_hidden = tf.nn.relu(test_logits1)\n",
    "  test_logits2 = tf.matmul(test_hidden, weights2) + biases2\n",
    "  test_prediction = tf.nn.softmax(test_logits2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let us run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 357.200989\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 19.2%\n",
      "Minibatch loss at step 500: 25.158081\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 1000: 13.223763\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 1500: 11.892635\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2000: 9.444946\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 2500: 8.246615\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 3000: 10.184418\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 83.1%\n",
      "Test accuracy: 90.1%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fefdb6874d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "valid_accuracy = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      \n",
    "      valid_accuracy1 = accuracy(valid_prediction.eval(), valid_labels)\n",
    "      valid_accuracy.append(valid_accuracy1)\n",
    "      print(\"Validation accuracy: %.1f%%\" % valid_accuracy1)\n",
    "    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "xx = np.arange(len(valid_accuracy))\n",
    "x_axis = [x*500 for x in xx]\n",
    "plt.plot(x_axis, valid_accuracy, '-.')\n",
    "plt.xlabel(\"learning steps\")\n",
    "plt.ylabel(\"Validation accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
